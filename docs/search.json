[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! My name is Yagmur Yunus ASLAN.\nThis is my personal webpage.\nPlease stay tuned to follow my works on data analytics, blog posts, and more.\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "",
    "text": "Welcome to my project page.\nKeep an eye on this space to stay updated with my project activities."
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\n\nCousin Marriage Data\nExported from the Turkish Statistical Institute database, this table contains, for each province and year, the total number of marriages, the number of marriages between first cousins, and the proportion of cousin marriages (%). Coverage: 2010-2024.\nEducation Level Data:\nAlso from TUIK, this table provides the count of individuals by education level (illiterate, literate without diploma, primary school, secondary school, high school & equivalents, universities, master, doctorate) and sex, for each province and year. Coverage: 2010-2023."
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.2 General Information About Data",
    "text": "2.2 General Information About Data\n\nCousin Marriage Dataset\n\nColumns per year: total marriages, cousin marriages, and cousin-marriage proportion (%)\n\nRows: 81 provinces/14 years\n\nEducation Dataset\n\nColumns: year, province code & name, total population by education level and sex\n\nEducation levels: illiterate, literate without diploma, primary, lower secondary, high school, universities, master, doctorate,unknown\n\nRows: 81 provinces /8 education categories with 2 sexes / 15 years"
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.3 Reason of Choice",
    "text": "2.3 Reason of Choice\nchose these data sets because it is necessary to examine the level of education from both a cultural and social (inbreeding) point of view. Demonstrating an inverse relationship between education levels and cousin-marriage rates may support targeted interventions in areas with low education levels. Moreover, provincial granularity offers insights into regional policy needs."
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.4 Preprocessing",
    "text": "2.4 Preprocessing\n\nLoading & Header Processing\n\nRead both Excel files using readxl::read_excel(..., col_names = FALSE) to preserve raw header rows.\n\n\nlibrary(readxl)\n\nWarning: package 'readxl' was built under R version 4.4.3\n\nakraba_raw &lt;- read_excel(\"akraba_evliligi.xlsx\",\n                         col_names = FALSE)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n• `` -&gt; `...58`\n• `` -&gt; `...59`\n• `` -&gt; `...60`\n\negitim_raw &lt;- read_excel(\"egitim_durumu.xlsx\",\n                         col_names = FALSE)\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n\n\n\nWhen the data obtained from TUIK are examined without manipulation, it is seen that its structure is not suitable for reading in R.\nI have brought two data into tidy form by making Reshaping to make it tidy\n\nEducation Data Tidy\nClean headers and reshape education data into (Province, Year, Education_level_sex,)\nCousin Marriage Data Tidy Similarly clean headers and reshape cousin marriage data into (Province, Year, Number of marriages, Number of marriages between first cousins, Proportion of marriages between first cousins (%))\n\n\ntidy_education &lt;- read_excel(\"tidy_education.xlsx\")\n\ntidy_marriages &lt;- read_excel(\"tidy_marriages.xlsx\")\n\nData recorded as R.Data and column names are shown.\n\nsave(tidy_marriages, tidy_education, file = \"data.RData\")\n  \n# tidy_marriages\nprint(names(tidy_marriages))\n\n[1] \"Province\"                                                     \n[2] \"Year\"                                                         \n[3] \"Number of\\r\\nmarriages\"                                       \n[4] \"Number of\\r\\nmarriages\\r\\nbetween\\r\\nfirst cousins\"           \n[5] \"Proportion\\r\\nof marriages\\r\\nbetween\\r\\nfirst cousins\\r\\n(%)\"\n\n# tidy_education\nprint(names(tidy_education))\n\n [1] \"Province\"                        \"Year\"                           \n [3] \"Illiterate_Total\"                \"Illiterate_Male\"                \n [5] \"Illiterate_Female\"               \"Literate_without_diploma_Total\" \n [7] \"Literate_without_diploma_Male\"   \"Literate_without_diploma_Female\"\n [9] \"Primary1_school_Total\"           \"Primary1_school_Male\"           \n[11] \"Primary1_school_Female\"          \"Primary2_school_Total\"          \n[13] \"Primary2_school_Male\"            \"Primary2_school_Female\"         \n[15] \"Lower_secondary_school_Total\"    \"Lower_secondary_school_Male\"    \n[17] \"Lower_secondary_school_Female\"   \"High_school_Total\"              \n[19] \"High_school_Male\"                \"High_school_Female\"             \n[21] \"Universities_Total\"              \"Universities_Male\"              \n[23] \"Universities_Female\"             \"Master_Total\"                   \n[25] \"Master_Male\"                     \"Master_Female\"                  \n[27] \"Doctorate_Total\"                 \"Doctorate_Male\"                 \n[29] \"Doctorate_Female\"                \"Unknown_Total\"                  \n[31] \"Unknown_Male\"                    \"Unknown_Female\""
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis\n\nmarriage_years  &lt;- sort(unique(tidy_marriages$Year))\neducation_years &lt;- sort(unique(tidy_education$Year))\n\ncat(\"Marriages data covers years:\", marriage_years, \"\\n\")\n\nMarriages data covers years: 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 \n\ncat(\"Education data covers years:\", education_years, \"\\n\")\n\nEducation data covers years: 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n\n\n\n# --- Provinces ---\nprovinces &lt;- sort(unique(tidy_marriages$Province))\ncat(\"Number of provinces in marriages data:\", length(provinces), \"\\n\")\n\nNumber of provinces in marriages data: 82 \n\ncat(\"Sample provinces:\", paste(head(provinces, 10), collapse = \", \"), \"...\\n\\n\")\n\nSample provinces: Adana, Adıyaman, Afyonkarahisar, Ağrı, Aksaray, Amasya, Ankara, Antalya, Ardahan, Artvin ...\n\n# --- Education Levels  ---\nedu_cols   &lt;- names(tidy_education)[-(1:2)]\nedu_levels &lt;- unique(sub(\"_(Total|Male|Female)$\", \"\", edu_cols))\ncat(\"Education levels available:\", paste(edu_levels, collapse = \", \"), \"\\n\")\n\nEducation levels available: Illiterate, Literate_without_diploma, Primary1_school, Primary2_school, Lower_secondary_school, High_school, Universities, Master, Doctorate, Unknown"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis\nxxxx"
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting\nxxxxxx"
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.4 Results",
    "text": "3.4 Results\nxxxxxx"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the Spring 2024-2025 EMU660 Decision Making with Analytics course.\nPlease use left menu to navigate through my assignments.\n\n\n\n Back to top",
    "crumbs": [
      "My Assignments"
    ]
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nRoketsan, Engineer, 2024 - ongoing\nBeko, Engineer, 2024"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nHavelsan, Intern, 2022\nRoketsan, Intern, 2023"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/assignments/assignment-1.html",
    "href": "docs/assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nMy first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has three parts.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "YagmurYunusASLAN.html",
    "href": "YagmurYunusASLAN.html",
    "title": "Welcome to My Analytics Lab, Please Take a Seat",
    "section": "",
    "text": "My name is Yagmur Yunus ASLAN.\nIn short I,\nAfter graduating from TOBB University of Economics and Technology with a grade point average of 3.53, I worked as a long-term intern at Havelsan and Roketsan for about 1 year, participating in business process modeling, defense industry, and space systems projects. Then I am now an engineer at Roketsan and studying MSc at Hacettepe University. I am proficient at the intermediate-upper level in English, and I have Spanish proficiency at the basic level. As an engineer constantly learning and improving himself, I am excellent at analytical thinking and problem-solving.\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "# Education"
  },
  {
    "objectID": "assignments/assignment-1.html#achoose-a-video-from-one-of-the-following-sources-and-provide-a-brief-summary",
    "href": "assignments/assignment-1.html#achoose-a-video-from-one-of-the-following-sources-and-provide-a-brief-summary",
    "title": "Assignment 1",
    "section": "(a)Choose a video from one of the following sources and provide a brief summary",
    "text": "(a)Choose a video from one of the following sources and provide a brief summary\nI chose “Veri Bilimi ve Endustri Muhendisligi Uzerine Sohbetler - Mustafa Baydogan & Erdi Dasdemir”\nAssoc. Prof. Dr. Mustafa Gokce Baydogan participated as a guest in the EMU430 Data Analytics course, which was newly added to the curriculum. He is a faculty member in the Department of Industrial Engineering at Bogazici University and the founder of Algopoly Software and Consultancy Joint Stock Company, based in Istanbul. With a strong academic background, he has worked on large-scale data mining, time series analysis, pattern exploration, and operations research using data science tools.\nTo convey his perspective to the students, he shared his answer to the question “Can the curvature of wet timber be estimated from its pictures?”, a problem he had worked on before.\nIn order to prevent the timber from being patched after the drying process, a pre-drying action is planned based on analyzing the wet timber images using image processing and machine learning techniques. The straight version of the timber used in home construction can be sold for 10 dollars, while the curved version is sold for only 2 dollars. If this problem is solved, a 5% increase in income is targeted.\nWhen addressing the problem, the root causes of warping were first investigated. Then, the factors that could cause warping were extracted from pictures taken before drying. These factors were processed and fed into learning algorithms, enabling the identification of timber pieces likely to warp. Filtering techniques helped interpret the locations and causes of warping.\nNext, to explain the importance of forecast accuracy in decision-making, he discussed how incorrect consumption forecasts in the electricity market can cause imbalances and how these effects can be mitigated.\nFollowing this, he explored the working principles of product ranking algorithms designed by e-commerce companies to optimize their revenue. The proposed ranking system arranges products based on the probability of purchase according to the buyer’s profile, and the discussion included how different ranking algorithms could be incorporated.\nFinally, to broaden students’ perspectives, he introduced physics-informed machine learning and machine learning for optimization.\nThe lesson concluded with a Q&A session.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#bexplore-statistical-summaries-with-custom-functions-and-iteration-methods",
    "href": "assignments/assignment-1.html#bexplore-statistical-summaries-with-custom-functions-and-iteration-methods",
    "title": "Assignment 1",
    "section": "(b)Explore Statistical Summaries with Custom Functions and Iteration Methods",
    "text": "(b)Explore Statistical Summaries with Custom Functions and Iteration Methods\nFirstly write a custom summary function. The function should be named compute_stats. It should take a numeric vector as input. The function should return a named list containing the mean, median, variance, interquartile range (IQR), minimum, and maximum of the input.\n\ncompute_stats&lt;- function(x){\n  if(is.numeric(x)){\n    k&lt;- list(\n      MEAN = mean(x),\n      MEDIAN = median(x),\n      VARIANCE = var(x),\n      IQR = IQR(x),\n      MIN = min(x),\n      MAX = max(x)\n      )\n    return (k)\n  }else print(\"Input was not numeric\")\n}\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\ncompute_stats(mtcars$mpg)\n\n$MEAN\n[1] 20.09062\n\n$MEDIAN\n[1] 19.2\n\n$VARIANCE\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$MIN\n[1] 10.4\n\n$MAX\n[1] 33.9\n\n\nSecondly, Applying the Function Using a Loop: Use a for loop to iterate over all numerical columns of the dataset. Within the loop, apply the compute_stats function to each column. Print the computed statistics, ensuring the column name appears in the output.\n\nfor(colonnames in colnames(mtcars)){\n  \n  colondata &lt;- mtcars[[colonnames]]\n  \n  print(colonnames)\n  print(compute_stats(colondata))\n  \n}\n\n[1] \"mpg\"\n$MEAN\n[1] 20.09062\n\n$MEDIAN\n[1] 19.2\n\n$VARIANCE\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$MIN\n[1] 10.4\n\n$MAX\n[1] 33.9\n\n[1] \"cyl\"\n$MEAN\n[1] 6.1875\n\n$MEDIAN\n[1] 6\n\n$VARIANCE\n[1] 3.189516\n\n$IQR\n[1] 4\n\n$MIN\n[1] 4\n\n$MAX\n[1] 8\n\n[1] \"disp\"\n$MEAN\n[1] 230.7219\n\n$MEDIAN\n[1] 196.3\n\n$VARIANCE\n[1] 15360.8\n\n$IQR\n[1] 205.175\n\n$MIN\n[1] 71.1\n\n$MAX\n[1] 472\n\n[1] \"hp\"\n$MEAN\n[1] 146.6875\n\n$MEDIAN\n[1] 123\n\n$VARIANCE\n[1] 4700.867\n\n$IQR\n[1] 83.5\n\n$MIN\n[1] 52\n\n$MAX\n[1] 335\n\n[1] \"drat\"\n$MEAN\n[1] 3.596563\n\n$MEDIAN\n[1] 3.695\n\n$VARIANCE\n[1] 0.2858814\n\n$IQR\n[1] 0.84\n\n$MIN\n[1] 2.76\n\n$MAX\n[1] 4.93\n\n[1] \"wt\"\n$MEAN\n[1] 3.21725\n\n$MEDIAN\n[1] 3.325\n\n$VARIANCE\n[1] 0.957379\n\n$IQR\n[1] 1.02875\n\n$MIN\n[1] 1.513\n\n$MAX\n[1] 5.424\n\n[1] \"qsec\"\n$MEAN\n[1] 17.84875\n\n$MEDIAN\n[1] 17.71\n\n$VARIANCE\n[1] 3.193166\n\n$IQR\n[1] 2.0075\n\n$MIN\n[1] 14.5\n\n$MAX\n[1] 22.9\n\n[1] \"vs\"\n$MEAN\n[1] 0.4375\n\n$MEDIAN\n[1] 0\n\n$VARIANCE\n[1] 0.2540323\n\n$IQR\n[1] 1\n\n$MIN\n[1] 0\n\n$MAX\n[1] 1\n\n[1] \"am\"\n$MEAN\n[1] 0.40625\n\n$MEDIAN\n[1] 0\n\n$VARIANCE\n[1] 0.2489919\n\n$IQR\n[1] 1\n\n$MIN\n[1] 0\n\n$MAX\n[1] 1\n\n[1] \"gear\"\n$MEAN\n[1] 3.6875\n\n$MEDIAN\n[1] 4\n\n$VARIANCE\n[1] 0.5443548\n\n$IQR\n[1] 1\n\n$MIN\n[1] 3\n\n$MAX\n[1] 5\n\n[1] \"carb\"\n$MEAN\n[1] 2.8125\n\n$MEDIAN\n[1] 2\n\n$VARIANCE\n[1] 2.608871\n\n$IQR\n[1] 2\n\n$MIN\n[1] 1\n\n$MAX\n[1] 8\n\n\nLastly, analternative approach with sapply and apply: Instead of a for loop, use the sapply function to apply compute_stats across all numerical columns.\n\nusing_sapply &lt;- sapply(mtcars, function(col) {\n  if (is.numeric(col)){\n    compute_stats(col)\n  }\n})\nusing_sapply\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nMEAN     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nMEDIAN   19.2     6        196.3    123      3.695     3.325    17.71   \nVARIANCE 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nMIN      10.4     4        71.1     52       2.76      1.513    14.5    \nMAX      33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nMEAN     0.4375    0.40625   3.6875    2.8125  \nMEDIAN   0         0         4         2       \nVARIANCE 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nMIN      0         0         3         1       \nMAX      1         1         5         8       \n\n\nUse the apply function with the appropriate margin to apply your function across all columns of the matrix.\n\nmatrix &lt;- as.matrix(mtcars)\n\nusing_apply &lt;- apply(matrix, 2, compute_stats)\n\nusing_apply\n\n$mpg\n$mpg$MEAN\n[1] 20.09062\n\n$mpg$MEDIAN\n[1] 19.2\n\n$mpg$VARIANCE\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$MIN\n[1] 10.4\n\n$mpg$MAX\n[1] 33.9\n\n\n$cyl\n$cyl$MEAN\n[1] 6.1875\n\n$cyl$MEDIAN\n[1] 6\n\n$cyl$VARIANCE\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$MIN\n[1] 4\n\n$cyl$MAX\n[1] 8\n\n\n$disp\n$disp$MEAN\n[1] 230.7219\n\n$disp$MEDIAN\n[1] 196.3\n\n$disp$VARIANCE\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$MIN\n[1] 71.1\n\n$disp$MAX\n[1] 472\n\n\n$hp\n$hp$MEAN\n[1] 146.6875\n\n$hp$MEDIAN\n[1] 123\n\n$hp$VARIANCE\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$MIN\n[1] 52\n\n$hp$MAX\n[1] 335\n\n\n$drat\n$drat$MEAN\n[1] 3.596563\n\n$drat$MEDIAN\n[1] 3.695\n\n$drat$VARIANCE\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$MIN\n[1] 2.76\n\n$drat$MAX\n[1] 4.93\n\n\n$wt\n$wt$MEAN\n[1] 3.21725\n\n$wt$MEDIAN\n[1] 3.325\n\n$wt$VARIANCE\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$MIN\n[1] 1.513\n\n$wt$MAX\n[1] 5.424\n\n\n$qsec\n$qsec$MEAN\n[1] 17.84875\n\n$qsec$MEDIAN\n[1] 17.71\n\n$qsec$VARIANCE\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$MIN\n[1] 14.5\n\n$qsec$MAX\n[1] 22.9\n\n\n$vs\n$vs$MEAN\n[1] 0.4375\n\n$vs$MEDIAN\n[1] 0\n\n$vs$VARIANCE\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$MIN\n[1] 0\n\n$vs$MAX\n[1] 1\n\n\n$am\n$am$MEAN\n[1] 0.40625\n\n$am$MEDIAN\n[1] 0\n\n$am$VARIANCE\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$MIN\n[1] 0\n\n$am$MAX\n[1] 1\n\n\n$gear\n$gear$MEAN\n[1] 3.6875\n\n$gear$MEDIAN\n[1] 4\n\n$gear$VARIANCE\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$MIN\n[1] 3\n\n$gear$MAX\n[1] 5\n\n\n$carb\n$carb$MEAN\n[1] 2.8125\n\n$carb$MEDIAN\n[1] 2\n\n$carb$VARIANCE\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$MIN\n[1] 1\n\n$carb$MAX\n[1] 8",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c",
    "href": "assignments/assignment-1.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#cload-the-na_example-dataset-from-the-dslabs-package.",
    "href": "assignments/assignment-1.html#cload-the-na_example-dataset-from-the-dslabs-package.",
    "title": "Assignment 1",
    "section": "(c)Load the ???na_example??? dataset from the dslabs package.",
    "text": "(c)Load the ???na_example??? dataset from the dslabs package.\nDisplay the dataset contents , including any NA (missing) values present.\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nReport the total count of NA values found within the dataset and the index positions of all NA values in the dataset.\n\nsum(is.na(na_example))\n\n[1] 145\n\nwhich(is.na(na_example))\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n\nCompute and display the mean and standard deviation of the dataset before handling missing values (ignore NAs in calculations)\n\nmean(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 2.301754\n\nsd(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 1.22338\n\n\nHandling Missing Values- create two modified versions of the dataset:\nVersion 1: Replace all NA values with the median of the non-missing values.\n\nversion1&lt;-replace(na_example,which(is.na(na_example)),median(na_example,na.rm = TRUE))\nversion1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nVersion 2: Replace all NA values with a randomly selected non-missing value from the dataset.\n\nna_remove &lt;- na_example[which(!is.na(na_example))]\nversion2&lt;-replace(na_example,which(is.na(na_example)),sample(na_remove, sum(is.na(na_example))))\nversion2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 1 2 2 1 4 1 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 2 4 4 7 3 2 2 3 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 1 3 1 1 1 5 1 2 1 3 5 3 2 2 2 2 6 3 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 3 1 1 2 2 1 3 1 1 5 2 1 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 4 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 1 2 1 1 4 3 5 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 2 3 3 1 1 1 5 2 3 3 2 4 1 3 2 5 1 2 3\n [260] 4 6 2 2 2 2 2 1 2 6 3 3 2 2 4 3 1 4 2 1 2 4 3 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 1 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 3 1 2 1 1 3 3 1 1 1 3 5 3 2 3 4 1 4 3 1 1 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 3 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 1 3 3 1 2 4 1 2 1 1 4 2 1 4 4 5\n [408] 1 2 5 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 1 4 4 4 1 1 1 4\n [445] 3 5 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 1 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 1 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 3 1 4 1 2 4 1 3 2 1 1 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 1 2 3 2 1 2 1 1 1 2 2 3 1 5 2 1 2 5 3 2 2 2 1 5 3 2 3 1 1 3 1 2\n [593] 2 2 1 2 2 4 3 6 1 2 3 1 1 2 2 3 3 3 2 3 3 4 2 1 2 2 4 3 1 1 2 2 3 1 1 1 3\n [630] 3 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 3 1 4 3 2 2 2 3 2 4 2 2 4 3\n [667] 2 5 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 5 1 5 1 1 2 6 2 3 4 1 3 1 2 1 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 4 3 2 4 3 3 3\n [741] 2 4 2 4 2 3 4 2 2 1 2 2 4 3 2 1 2 3 1 3 4 2 1 2 1 2 1 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 4 1 3 1 2 3 1 2 1 2 1 3 1 3 2 3 2 2 2 1 4 2 1 1 2 2 4 2 3 2 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 1 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 4 3\n [889] 3 2 2 7 1 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 3 1 1 3 2 1 1 1 2 2 2 2 3 3 2 1 3\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 4 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 4 3 2 1 3 1 1 2 5 3 5 1 1 4 3 2 1 3 1 1 2 4 3 3 3 3 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nCompare the Results: Compute the mean and standard deviation of both modified datasets. Compare these statistics with those from the original dataset (before handling NAs). Briefly explain which method seems more appropriate for handling missing data in this case.\n\nsd(na_example, na.rm=TRUE)\n\n[1] 1.22338\n\nsd(version1)\n\n[1] 1.136102\n\nsd(version2)\n\n[1] 1.230249\n\n\nIt seems that it is more appropriate for handling missing data to write the most repeated value used in version 1 instead of NA, as it reduces the standard deviation of the data.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#cload-the-.na_example.-dataset-from-the-dslabs-package.",
    "href": "assignments/assignment-1.html#cload-the-.na_example.-dataset-from-the-dslabs-package.",
    "title": "Assignment 1",
    "section": "(c)Load the .na_example. dataset from the dslabs package.",
    "text": "(c)Load the .na_example. dataset from the dslabs package.\nDisplay the dataset contents , including any NA (missing) values present.\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nReport the total count of NA values found within the dataset and the index positions of all NA values in the dataset.\n\nsum(is.na(na_example))\n\n[1] 145\n\nwhich(is.na(na_example))\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n\nCompute and display the mean and standard deviation of the dataset before handling missing values (ignore NAs in calculations)\n\nmean(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 2.301754\n\nsd(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 1.22338\n\n\nHandling Missing Values- create two modified versions of the dataset:\nVersion 1: Replace all NA values with the median of the non-missing values.\n\nversion1&lt;-replace(na_example,which(is.na(na_example)),median(na_example,na.rm = TRUE))\nversion1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nVersion 2: Replace all NA values with a randomly selected non-missing value from the dataset.\n\nna_remove &lt;- na_example[which(!is.na(na_example))]\nversion2&lt;-replace(na_example,which(is.na(na_example)),sample(na_remove, sum(is.na(na_example))))\nversion2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 3 2 2 1 4 1 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 1 2 1 1 1 5 1 3 1 2 4 4 7 3 2 1 4 1 3 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 5 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 1 4 3 4 3 1 2 1 2 3 2 2 1 5 1 2 1 3 5 3 2 2 1 5 1 2 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 3 1 1 2 2 1 3 1 1 5 2 1 2 4 3 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 1 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 3 1 3 1 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 5 1 4 2 3 3 3 1 1 5 2 3 3 2 4 2 3 2 5 3 2 3\n [260] 4 6 2 2 2 1 2 1 2 6 3 3 2 2 4 3 1 4 2 3 2 4 1 6 2 3 1 1 2 2 3 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 3 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 4 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 3 2 2 1 2 1 1 4 2 1 4 4 3\n [408] 1 2 1 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 3 4 4 4 1 1 3 4\n [445] 3 1 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 1 3 1 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 1 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 4 1 4 1 2 4 1 3 2 3 1 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 7 1 1\n [556] 3 1 3 2 4 1 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 3 3 2 2 2 1 5 3 2 3 1 4 3 1 2\n [593] 2 2 1 2 2 4 3 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 2 2 5 4 2 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 2 7 1 3 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 1 1 4 3 2 2 2 3 2 4 2 2 4 4\n [667] 1 1 6 3 3 1 4 4 2 1 4 1 6 3 3 3 2 1 1 6 3 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 2 4 4 3 3 3\n [741] 2 4 2 4 2 2 3 3 2 1 1 2 4 3 2 1 2 3 1 3 4 3 1 2 1 2 1 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 1 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 4 1 2 4 2 4 1 3\n [852] 1 2 5 5 2 2 2 3 2 1 3 1 3 2 4 2 4 1 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 1 3\n [889] 3 2 2 1 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 3 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 3\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 1\n [963] 1 2 3 1 2 3 3 1 1 2 5 3 5 1 1 4 3 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 5\n[1000] 2\n\n\nCompare the Results: Compute the mean and standard deviation of both modified datasets. Compare these statistics with those from the original dataset (before handling NAs). Briefly explain which method seems more appropriate for handling missing data in this case.\n\nsd(na_example, na.rm=TRUE)\n\n[1] 1.22338\n\nsd(version1)\n\n[1] 1.136102\n\nsd(version2)\n\n[1] 1.216831\n\n\nIt seems that it is more appropriate for handling missing data to write the most repeated value used in version 1 instead of NA, as it reduces the standard deviation of the data.",
    "crumbs": [
      "Assignment 1"
    ]
  }
]