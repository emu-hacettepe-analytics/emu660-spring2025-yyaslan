[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytics Lab",
    "section": "",
    "text": "Hello!! My name is Yagmur Yunus ASLAN.\nThis is my personal webpage.\nPlease stay tuned to follow my works on data analytics, blog posts, and more.\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "",
    "text": "Welcome to my project page.\nKeep an eye on this space to stay updated with my project activities."
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\n\nCousin Marriage Data\nExported from the Turkish Statistical Institute database, this table contains, for each province and year, the total number of marriages, the number of marriages between first cousins, and the proportion of cousin marriages (%). Coverage: 2010-2024.\nEducation Level Data:\nAlso from TUIK, this table provides the count of individuals by education level (illiterate, literate without diploma, primary school, secondary school, high school & equivalents, universities, master, doctorate) and sex, for each province and year. Coverage: 2010-2023."
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.2 General Information About Data",
    "text": "2.2 General Information About Data\n\nCousin Marriage Dataset\n\nColumns per year: total marriages, cousin marriages, and cousin-marriage proportion (%)\n\nRows: 81 provinces/14 years\n\nEducation Dataset\n\nColumns: year, province code & name, total population by education level and sex\n\nEducation levels: illiterate, literate without diploma, primary, lower secondary, high school, universities, master, doctorate,unknown\n\nRows: 81 provinces /8 education categories with 2 sexes / 15 years"
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.3 Reason of Choice",
    "text": "2.3 Reason of Choice\nchose these data sets because it is necessary to examine the level of education from both a cultural and social (inbreeding) point of view. Demonstrating an inverse relationship between education levels and cousin-marriage rates may support targeted interventions in areas with low education levels. Moreover, provincial granularity offers insights into regional policy needs."
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "2.4 Preprocessing",
    "text": "2.4 Preprocessing\n\nLoading & Header Processing\n\nRead both Excel files using readxl to preserve raw header rows.\n\n\n\nShow the code\nlibrary(readxl)\nakraba_raw &lt;- read_excel(\"akraba_evliligi.xlsx\",\n                         col_names = FALSE)\n\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n• `` -&gt; `...50`\n• `` -&gt; `...51`\n• `` -&gt; `...52`\n• `` -&gt; `...53`\n• `` -&gt; `...54`\n• `` -&gt; `...55`\n• `` -&gt; `...56`\n• `` -&gt; `...57`\n• `` -&gt; `...58`\n• `` -&gt; `...59`\n• `` -&gt; `...60`\n\n\nShow the code\negitim_raw &lt;- read_excel(\"egitim_durumu.xlsx\",\n                         col_names = FALSE)\n\n\nNew names:\n• `` -&gt; `...1`\n• `` -&gt; `...2`\n• `` -&gt; `...3`\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n• `` -&gt; `...6`\n• `` -&gt; `...7`\n• `` -&gt; `...8`\n• `` -&gt; `...9`\n• `` -&gt; `...10`\n• `` -&gt; `...11`\n• `` -&gt; `...12`\n• `` -&gt; `...13`\n• `` -&gt; `...14`\n• `` -&gt; `...15`\n• `` -&gt; `...16`\n• `` -&gt; `...17`\n• `` -&gt; `...18`\n• `` -&gt; `...19`\n• `` -&gt; `...20`\n• `` -&gt; `...21`\n• `` -&gt; `...22`\n• `` -&gt; `...23`\n• `` -&gt; `...24`\n• `` -&gt; `...25`\n• `` -&gt; `...26`\n• `` -&gt; `...27`\n• `` -&gt; `...28`\n• `` -&gt; `...29`\n• `` -&gt; `...30`\n• `` -&gt; `...31`\n• `` -&gt; `...32`\n• `` -&gt; `...33`\n• `` -&gt; `...34`\n• `` -&gt; `...35`\n• `` -&gt; `...36`\n• `` -&gt; `...37`\n• `` -&gt; `...38`\n• `` -&gt; `...39`\n• `` -&gt; `...40`\n• `` -&gt; `...41`\n• `` -&gt; `...42`\n• `` -&gt; `...43`\n• `` -&gt; `...44`\n• `` -&gt; `...45`\n• `` -&gt; `...46`\n• `` -&gt; `...47`\n• `` -&gt; `...48`\n• `` -&gt; `...49`\n\n\n\nWhen the data obtained from TUIK are examined without manipulation, it is seen that its structure is not suitable for reading in R.\nI have brought two data into tidy form by making Reshaping to make it tidy\n\nEducation Data Tidy\nClean headers and reshape education data into (Province, Year, Education_level_sex,)\nCousin Marriage Data Tidy Similarly clean headers and reshape cousin marriage data into (Province, Year, Number of marriages, Number of marriages between first cousins, Proportion of marriages between first cousins (%))\n\n\n\nShow the code\ntidy_education &lt;- read_excel(\"tidy_education.xlsx\")\n\ntidy_marriages &lt;- read_excel(\"tidy_marriages.xlsx\")\n\n\nData recorded as R.Data and column names are shown.\n\n\nShow the code\nsave(tidy_marriages, tidy_education, file = \"data.RData\")\n  \n# tidy_marriages\nprint(names(tidy_marriages))\n\n\n[1] \"Province\"                                 \n[2] \"Year\"                                     \n[3] \"Number_of_marriages\"                      \n[4] \"Number_of_marriages_between_first_cousins\"\n[5] \"Proportion\"                               \n\n\nShow the code\n# tidy_education\nprint(names(tidy_education))\n\n\n [1] \"Province\"                        \"Year\"                           \n [3] \"Illiterate_Total\"                \"Illiterate_Male\"                \n [5] \"Illiterate_Female\"               \"Literate_without_diploma_Total\" \n [7] \"Literate_without_diploma_Male\"   \"Literate_without_diploma_Female\"\n [9] \"Primary1_school_Total\"           \"Primary1_school_Male\"           \n[11] \"Primary1_school_Female\"          \"Primary2_school_Total\"          \n[13] \"Primary2_school_Male\"            \"Primary2_school_Female\"         \n[15] \"Lower_secondary_school_Total\"    \"Lower_secondary_school_Male\"    \n[17] \"Lower_secondary_school_Female\"   \"High_school_Total\"              \n[19] \"High_school_Male\"                \"High_school_Female\"             \n[21] \"Universities_Total\"              \"Universities_Male\"              \n[23] \"Universities_Female\"             \"Master_Total\"                   \n[25] \"Master_Male\"                     \"Master_Female\"                  \n[27] \"Doctorate_Total\"                 \"Doctorate_Male\"                 \n[29] \"Doctorate_Female\"                \"Unknown_Total\"                  \n[31] \"Unknown_Male\"                    \"Unknown_Female\""
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis\n\n\nShow the code\nmarriage_years  &lt;- sort(unique(tidy_marriages$Year))\neducation_years &lt;- sort(unique(tidy_education$Year))\n\ncat(\"Marriages data covers years:\", marriage_years, \"\\n\")\n\n\nMarriages data covers years: 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 2024 \n\n\nShow the code\ncat(\"Education data covers years:\", education_years, \"\\n\")\n\n\nEducation data covers years: 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023 \n\n\n\n\nShow the code\n# --- Provinces ---\nprovinces &lt;- sort(unique(tidy_marriages$Province))\ncat(\"Number of provinces in marriages data:\", length(provinces), \"\\n\")\n\n\nNumber of provinces in marriages data: 82 \n\n\nShow the code\ncat(\"Sample provinces:\", paste(head(provinces, 10), collapse = \", \"), \"...\\n\\n\")\n\n\nSample provinces: Adana, Adıyaman, Afyonkarahisar, Ağrı, Aksaray, Amasya, Ankara, Antalya, Ardahan, Artvin ...\n\n\nShow the code\n# --- Education Levels  ---\nedu_cols   &lt;- names(tidy_education)[-(1:2)]\nedu_levels &lt;- unique(sub(\"_(Total|Male|Female)$\", \"\", edu_cols))\ncat(\"Education levels available:\", paste(edu_levels, collapse = \", \"), \"\\n\")\n\n\nEducation levels available: Illiterate, Literate_without_diploma, Primary1_school, Primary2_school, Lower_secondary_school, High_school, Universities, Master, Doctorate, Unknown"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis\nTidy data has been combined according to city and year to use the data more effectively.\n\n\nShow the code\ncombined_data &lt;- merge(tidy_education, tidy_marriages, by = c(\"Province\", \"Year\"))\n\n\nHow the educational levels of people in Turkey change by year is shown with line graphs.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nShow the code\nlibrary(tidyr)\nlibrary(stringi)\n\ncombined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  select(Year, Illiterate_Total, Literate_without_diploma_Total, \n         Primary1_school_Total, Primary2_school_Total, \n         Lower_secondary_school_Total, High_school_Total, \n         Universities_Total, Master_Total, Doctorate_Total) |&gt;\n  mutate(Year = as.numeric(Year)) |&gt;\n  pivot_longer(\n    cols = -Year,\n    names_to = \"Education_Level\",\n    values_to = \"People\"\n  ) |&gt;\n  filter(!is.na(People)) |&gt;\n  mutate(People = People / 1000) |&gt;\n  mutate(Education_Level = recode(Education_Level,\n                                  \"Illiterate_Total\" = \"Illiterate\",\n                                  \"Literate_without_diploma_Total\" = \"Literate_no_diploma\",\n                                  \"Primary1_school_Total\" = \"Primary 1\",\n                                  \"Primary2_school_Total\" = \"Primary 2\",\n                                  \"Lower_secondary_school_Total\" = \"Lower Secondary\",\n                                  \"High_school_Total\" = \"High School\",\n                                  \"Universities_Total\" = \"University\",\n                                  \"Master_Total\" = \"Master\",\n                                  \"Doctorate_Total\" = \"Doctorate\")) |&gt;\n  mutate(Education_Level = factor(Education_Level, levels = c(\n    \"Illiterate\", \"Literate_no_diploma\", \"Primary 1\", \"Primary 2\", \n    \"Lower Secondary\", \"High School\", \"University\", \"Master\", \"Doctorate\"\n  ))) |&gt;\n  ggplot(aes(x = Year, y = People)) +\n  geom_line() +\n  facet_wrap(~ Education_Level, scales = \"free_y\", nrow = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, by = 2)) +\n  labs(title = \"Turkey Education Levels - Small Multiple Plots\",\n       x = \"Year\", y = \"People (Thousands)\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nIn the same way, it was shown how the percentage of first cousin marriages in Turkey varies by year.\n\n\nShow the code\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringi)\n\ncombined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(Year = as.numeric(Year)) |&gt;\n  ggplot(aes(x = Year, y = Proportion)) +\n  geom_point(color = \"steelblue\") +\n  geom_text(aes(label = paste0(round(Proportion, 2), \"%\")),hjust=-0.3, vjust =-0.3, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, by = 2)) +\n  labs(title = \"Turkey - First Cousin Marriage Percentage Over Years\",\n       x = \"Year\", y = \"Percentage\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n\n\n\n\n\n\n\n\nThe increase after 2020 is thought to be because people could not socialize after the global pandemic and spent time with their closest acquaintances."
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting\nTo observe the compatibility of the two data models, first of all, the data according to the increase in the level of education for Turkey.\n\n3.3.1 Illiterate Population & Cousin Marriage Rate\nThe data suggests a possible positive association between the proportion of illiterate individuals and cousin marriage rates in Turkey. As literacy improves over time, a downward shift in consanguineous marriage patterns can be observed.\n\n\nShow the code\nedu_var &lt;- \"Illiterate_Total\"\n\nplot_data &lt;- combined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(Year = as.numeric(Year),\n         Education_k = .data[[edu_var]] / 1000,\n         Proportion_percent = Proportion) |&gt;\n  select(Year, Education_k, Proportion_percent)\n\n\nmin_y &lt;- min(plot_data$Education_k)\nmax_y &lt;- max(plot_data$Education_k)\n\nplot_data &lt;- plot_data |&gt;\n  mutate(Proportion_scaled = ((Proportion_percent - min(Proportion_percent)) /\n                              (max(Proportion_percent) - min(Proportion_percent))) *\n                             (max_y - min_y) + min_y)\n\n\nggplot(plot_data, aes(x = Year)) +\n  geom_line(aes(y = Education_k, color = \"Illiterate (1000s)\"), size = 1.2) +\n  geom_point(aes(y = Proportion_scaled, color = \"Cousin Marriage (%)\"), size = 2.5) +\n  geom_text(aes(y = Proportion_scaled, label = paste0(round(Proportion_percent, 2), \"%\")),\n            vjust = -0.8, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, 2)) +\n  scale_y_continuous(\n    name = \"Illiterate Individuals (1000s)\",\n    sec.axis = sec_axis(\n      trans = ~ (.-min_y) * (max(plot_data$Proportion_percent) - min(plot_data$Proportion_percent)) /\n                      (max_y - min_y) + min(plot_data$Proportion_percent),\n      name = \"Cousin Marriage (%)\"\n    )\n  ) +\n  labs(\n    title = \"Illiterate Population & Cousin Marriage Rate (Turkey)\",\n    x = \"Year\", color = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `trans` argument of `sec_axis()` is deprecated as of ggplot2 3.5.0.\nℹ Please use the `transform` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Primary School Graduates & Cousin Marriage (%)\nThe expansion of primary education in Turkey has played a key role in shaping social behaviors. This chart shows that increases in primary school graduation are associated with reductions in cousin marriage rates over time.\n\n\nShow the code\nplot_data &lt;- combined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    Education_k = (Primary1_school_Total + Primary2_school_Total) / 1000,\n    Proportion_percent = Proportion\n  ) |&gt;\n  select(Year, Education_k, Proportion_percent)\n\nmin_y &lt;- min(plot_data$Education_k)\nmax_y &lt;- max(plot_data$Education_k)\n\nplot_data &lt;- plot_data |&gt;\n  mutate(Proportion_scaled = ((Proportion_percent - min(Proportion_percent)) /\n                              (max(Proportion_percent) - min(Proportion_percent))) *\n                             (max_y - min_y) + min_y)\n\n\nggplot(plot_data, aes(x = Year)) +\n  geom_line(aes(y = Education_k, color = \"Primary School (1000s)\"), size = 1.2) +\n  geom_point(aes(y = Proportion_scaled, color = \"Cousin Marriage (%)\"), size = 2.5) +\n  geom_text(aes(y = Proportion_scaled, label = paste0(round(Proportion_percent, 2), \"%\")),\n            vjust = -0.8, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, 2)) +\n  scale_y_continuous(\n    name = \"Primary School Graduates (1000s)\",\n    sec.axis = sec_axis(\n      trans = ~ (.-min_y) * (max(plot_data$Proportion_percent) - min(plot_data$Proportion_percent)) /\n                      (max_y - min_y) + min(plot_data$Proportion_percent),\n      name = \"Cousin Marriage (%)\"\n    )\n  ) +\n  labs(\n    title = \"Primary School Graduates & Cousin Marriage Rate (Turkey)\",\n    x = \"Year\", color = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Lower Secondary School Graduates & Cousin Marriage (%)\nIn Turkey, the transition to the 4+4+4 education model in the 2012-2013 academic year explains the acceleration increase in the graph.\n\n\nShow the code\nedu_var &lt;- \"Lower_secondary_school_Total\"\n\nplot_data &lt;- combined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    Education_k = .data[[edu_var]] / 1000,\n    Proportion_percent = Proportion\n  ) |&gt;\n  select(Year, Education_k, Proportion_percent)\n\nmin_y &lt;- min(plot_data$Education_k)\nmax_y &lt;- max(plot_data$Education_k)\n\nplot_data &lt;- plot_data |&gt;\n  mutate(Proportion_scaled = ((Proportion_percent - min(Proportion_percent)) /\n                              (max(Proportion_percent) - min(Proportion_percent))) *\n                             (max_y - min_y) + min_y)\n\nggplot(plot_data, aes(x = Year)) +\n  geom_line(aes(y = Education_k, color = \"Lower Secondary School (1000s)\"), size = 1.2) +\n  geom_point(aes(y = Proportion_scaled, color = \"Cousin Marriage (%)\"), size = 2.5) +\n  geom_text(aes(y = Proportion_scaled, label = paste0(round(Proportion_percent, 2), \"%\")),\n            vjust = -0.8, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, 2)) +\n  scale_y_continuous(\n    name = \"Lower Secondary School Graduates (1000s)\",\n    sec.axis = sec_axis(\n      trans = ~ (.-min_y) * (max(plot_data$Proportion_percent) - min(plot_data$Proportion_percent)) /\n                      (max_y - min_y) + min(plot_data$Proportion_percent),\n      name = \"Cousin Marriage (%)\"\n    )\n  ) +\n  labs(\n    title = \"Lower Secondary School Graduates & Cousin Marriage Rate (Turkey)\",\n    x = \"Year\", color = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.3.4 High School Graduates& Cousin Marriage (%)\nIn Turkey, as the number of high school graduates has increased steadily over the years, the rate of cousin marriages has shown a noticeable decline. This trend implies a possible inverse relationship between secondary education completion and consanguineous marriage practices.\n\n\nShow the code\nedu_var &lt;- \"High_school_Total\"\n\nplot_data &lt;- combined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    Education_k = .data[[edu_var]] / 1000,\n    Proportion_percent = Proportion\n  ) |&gt;\n  select(Year, Education_k, Proportion_percent)\n\nmin_y &lt;- min(plot_data$Education_k)\nmax_y &lt;- max(plot_data$Education_k)\n\nplot_data &lt;- plot_data |&gt;\n  mutate(Proportion_scaled = ((Proportion_percent - min(Proportion_percent)) /\n                              (max(Proportion_percent) - min(Proportion_percent))) *\n                             (max_y - min_y) + min_y)\n\nggplot(plot_data, aes(x = Year)) +\n  geom_line(aes(y = Education_k, color = \"High School (1000s)\"), size = 1.2) +\n  geom_point(aes(y = Proportion_scaled, color = \"Cousin Marriage (%)\"), size = 2.5) +\n  geom_text(aes(y = Proportion_scaled, label = paste0(round(Proportion_percent, 2), \"%\")),\n            vjust = -0.8, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, 2)) +\n  scale_y_continuous(\n    name = \"High School Graduates (1000s)\",\n    sec.axis = sec_axis(\n      trans = ~ (.-min_y) * (max(plot_data$Proportion_percent) - min(plot_data$Proportion_percent)) /\n                      (max_y - min_y) + min(plot_data$Proportion_percent),\n      name = \"Cousin Marriage (%)\"\n    )\n  ) +\n  labs(\n    title = \"High School Graduates & Cousin Marriage Rate (Turkey)\",\n    x = \"Year\", color = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.5.5 Universities Graduates & Cousin Marriage (%)\nOver the years, as the number of university graduates in Turkey increased, a steady decline in the rate of cousin marriages can be observed. This trend suggests a potential negative correlation between higher education attainment and consanguineous marriage preferences.\n\n\nShow the code\nedu_var &lt;- \"Universities_Total\"\n\nplot_data &lt;- combined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    Education_k = .data[[edu_var]] / 1000,\n    Proportion_percent = Proportion\n  ) |&gt;\n  select(Year, Education_k, Proportion_percent)\n\nmin_y &lt;- min(plot_data$Education_k)\nmax_y &lt;- max(plot_data$Education_k)\n\nplot_data &lt;- plot_data |&gt;\n  mutate(Proportion_scaled = ((Proportion_percent - min(Proportion_percent)) /\n                              (max(Proportion_percent) - min(Proportion_percent))) *\n                             (max_y - min_y) + min_y)\n\nggplot(plot_data, aes(x = Year)) +\n  geom_line(aes(y = Education_k, color = \"University (1000s)\"), size = 1.2) +\n  geom_point(aes(y = Proportion_scaled, color = \"Cousin Marriage (%)\"), size = 2.5) +\n  geom_text(aes(y = Proportion_scaled, label = paste0(round(Proportion_percent, 2), \"%\")),\n            vjust = -0.8, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, 2)) +\n  scale_y_continuous(\n    name = \"University Graduates (1000s)\",\n    sec.axis = sec_axis(\n      trans = ~ (.-min_y) * (max(plot_data$Proportion_percent) - min(plot_data$Proportion_percent)) /\n                      (max_y - min_y) + min(plot_data$Proportion_percent),\n      name = \"Cousin Marriage (%)\"\n    )\n  ) +\n  labs(\n    title = \"University Graduates & Cousin Marriage Rate (Turkey)\",\n    x = \"Year\", color = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.5.5 Masters Degree Graduates & Cousin Marriage (%)\nA rising trend in master???s degree graduations appears to align with a gradual decrease in cousin marriage rates. This relationship may indicate that individuals with postgraduate education are less likely to engage in consanguineous marriages.\n\n\nShow the code\nedu_var &lt;- \"Master_Total\"\n\nplot_data &lt;- combined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    Education_k = .data[[edu_var]] / 1000,\n    Proportion_percent = Proportion\n  ) |&gt;\n  select(Year, Education_k, Proportion_percent)\n\nmin_y &lt;- min(plot_data$Education_k)\nmax_y &lt;- max(plot_data$Education_k)\n\nplot_data &lt;- plot_data |&gt;\n  mutate(Proportion_scaled = ((Proportion_percent - min(Proportion_percent)) /\n                              (max(Proportion_percent) - min(Proportion_percent))) *\n                             (max_y - min_y) + min_y)\n\nggplot(plot_data, aes(x = Year)) +\n  geom_line(aes(y = Education_k, color = \"Master's Degree (1000s)\"), size = 1.2) +\n  geom_point(aes(y = Proportion_scaled, color = \"Cousin Marriage (%)\"), size = 2.5) +\n  geom_text(aes(y = Proportion_scaled, label = paste0(round(Proportion_percent, 2), \"%\")),\n            vjust = -0.8, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, 2)) +\n  scale_y_continuous(\n    name = \"Master's Degree Graduates (1000s)\",\n    sec.axis = sec_axis(\n      trans = ~ (.-min_y) * (max(plot_data$Proportion_percent) - min(plot_data$Proportion_percent)) /\n                      (max_y - min_y) + min(plot_data$Proportion_percent),\n      name = \"Cousin Marriage (%)\"\n    )\n  ) +\n  labs(\n    title = \"Master's Degree Graduates & Cousin Marriage Rate (Turkey)\",\n    x = \"Year\", color = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.5.6 Doctorate Graduates & Cousin Marriage (%)\nAs the number of individuals attaining doctoral degrees increases, cousin marriage rates show a significant downward trend. This supports the hypothesis that advanced education is inversely associated with traditional marriage practices such as consanguinity.\n\n\nShow the code\nedu_var &lt;- \"Doctorate_Total\"\n\nplot_data &lt;- combined_data |&gt;\n  mutate(Province = stri_trans_general(Province, \"Latin-ASCII\"),\n         Province = tolower(Province)) |&gt;\n  filter(Province == \"turkiye\") |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    Education_k = .data[[edu_var]] / 1000,\n    Proportion_percent = Proportion\n  ) |&gt;\n  select(Year, Education_k, Proportion_percent)\n\nmin_y &lt;- min(plot_data$Education_k)\nmax_y &lt;- max(plot_data$Education_k)\n\nplot_data &lt;- plot_data |&gt;\n  mutate(Proportion_scaled = ((Proportion_percent - min(Proportion_percent)) /\n                              (max(Proportion_percent) - min(Proportion_percent))) *\n                             (max_y - min_y) + min_y)\n\nggplot(plot_data, aes(x = Year)) +\n  geom_line(aes(y = Education_k, color = \"Doctorate (1000s)\"), size = 1.2) +\n  geom_point(aes(y = Proportion_scaled, color = \"Cousin Marriage (%)\"), size = 2.5) +\n  geom_text(aes(y = Proportion_scaled, label = paste0(round(Proportion_percent, 2), \"%\")),\n            vjust = -0.8, size = 3) +\n  scale_x_continuous(breaks = seq(2010, 2024, 2)) +\n  scale_y_continuous(\n    name = \"Doctorate Graduates (1000s)\",\n    sec.axis = sec_axis(\n      trans = ~ (.-min_y) * (max(plot_data$Proportion_percent) - min(plot_data$Proportion_percent)) /\n                      (max_y - min_y) + min(plot_data$Proportion_percent),\n      name = \"Cousin Marriage (%)\"\n    )\n  ) +\n  labs(\n    title = \"Doctorate Graduates & Cousin Marriage Rate (Turkey)\",\n    x = \"Year\", color = \"\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n3.5.7 Heat Maps of Education Levels in Turkey\nThe animated maps below illustrate the spatial distribution of high school and higher education graduates across Turkish provinces over time. Color gradients represent the number of graduates (in thousands), while the labels highlight the top five and bottom five provinces each year.\n\n3.5.7.1 High School Heat Map\n\n\nShow the code\nlibrary(readxl)\nlibrary(dplyr)\nlibrary(stringi)\nlibrary(ggplot2)\nlibrary(sf)\n\n\nLinking to GEOS 3.13.1, GDAL 3.10.2, PROJ 9.5.1; sf_use_s2() is TRUE\n\n\nShow the code\nlibrary(gganimate)\n\n\nturkiye &lt;- st_read(\"turkey-geojson.json\") |&gt;\n  mutate(il_adi = tolower(name),\n         il_adi = stri_trans_general(il_adi, \"Latin-ASCII\")) |&gt;\n  mutate(il_adi = case_when(\n    il_adi == \"afyon\" ~ \"afyonkarahisar\",\n    il_adi == \"eskisehir\" ~ \"eskisehir\",\n    il_adi == \"mugla\" ~ \"mugla\",\n    il_adi == \"sanliurfa\" ~ \"sanliurfa\",\n    il_adi == \"kirikkale\" ~ \"kirikkale\",\n    TRUE ~ il_adi\n  ))\n\n\nReading layer `turkey-geojson' from data source \n  `C:\\Users\\yagmu\\Desktop\\emu660-spring2025-yyaslan\\turkey-geojson.json' \n  using driver `GeoJSON'\nSimple feature collection with 81 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25.66514 ymin: 35.81543 xmax: 44.83384 ymax: 42.10541\nGeodetic CRS:  WGS 84\n\n\nShow the code\nedu &lt;- read_excel(\"tidy_education.xlsx\") |&gt;\n  mutate(\n    Province = stri_trans_general(Province, \"Latin-ASCII\"),\n    Province = tolower(trimws(Province))\n  ) |&gt;\n  filter(Province != \"turkiye\") |&gt;\n  select(Year, il_adi = Province, mezun = High_school_Total) |&gt;\n  mutate(mezun = mezun / 1000)\n\nharita_df &lt;- expand.grid(il_adi = unique(turkiye$il_adi), Year = unique(edu$Year)) |&gt;\n  left_join(edu, by = c(\"il_adi\", \"Year\")) |&gt;\n  left_join(turkiye, by = \"il_adi\") |&gt;\n  st_as_sf()\n\netiketler &lt;- harita_df |&gt;\n  group_by(Year) |&gt;\n  mutate(\n    sira = rank(mezun, ties.method = \"first\"),\n    etiket = case_when(\n      sira &lt;= 5 | sira &gt;= (n() - 4) ~ paste0(round(mezun), \"k\"),\n      TRUE ~ NA_character_\n    ),\n    x = st_coordinates(st_centroid(geometry))[,1],\n    y = st_coordinates(st_centroid(geometry))[,2]\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    x = ifelse(il_adi == \"istanbul\", x + 0.7, x),\n    y = ifelse(il_adi == \"istanbul\", y + 0.3, y)\n  )\n\n\np_high &lt;- ggplot(etiketler) +\n  geom_sf(aes(fill = mezun, geometry = geometry), color = \"white\", linewidth = 0.2) +\n  geom_text(data = subset(etiketler, !is.na(etiket)), \n            aes(x = x, y = y, label = etiket), \n            size = 3, color = \"black\") +\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, na.value = \"grey90\") +\n  labs(\n    title = \"{closest_state} High School Graduates (in Thousand)\",\n    fill = \"Thousand People\"\n  ) +\n  theme_minimal() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\nShow the code\nanim_save(\"lise.gif\", animate(p_high, width = 800, height = 600, fps = 2, duration = 10))\nlibrary(magick)\nimage_read(\"lise.gif\") %&gt;%\n  .[length(.)] %&gt;%  # son kare\n  image_convert(format = \"png\", type = \"truecolor\") %&gt;%\n  image_write(\"lise.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.7.2 University Heat Map\n\n\nShow the code\nedu_uni &lt;- read_excel(\"tidy_education.xlsx\") |&gt;\n  mutate(\n    Province = stri_trans_general(Province, \"Latin-ASCII\"),\n    Province = tolower(trimws(Province))\n  ) |&gt;\n  filter(Province != \"turkiye\") |&gt;\n  select(Year, il_adi = Province, mezun = Universities_Total) |&gt;\n  mutate(mezun = mezun / 1000)\n\n\nharita_uni &lt;- expand.grid(il_adi = unique(turkiye$il_adi), Year = unique(edu_uni$Year)) |&gt;\n  left_join(edu_uni, by = c(\"il_adi\", \"Year\")) |&gt;\n  left_join(turkiye, by = \"il_adi\") |&gt;\n  st_as_sf()\n\n\netiketler_uni &lt;- harita_uni |&gt;\n  group_by(Year) |&gt;\n  mutate(\n    sira = rank(mezun, ties.method = \"first\"),\n    etiket = case_when(\n      sira &lt;= 5 | sira &gt;= (n() - 4) ~ paste0(round(mezun), \"k\"),\n      TRUE ~ NA_character_\n    ),\n    x = st_coordinates(st_centroid(geometry))[,1],\n    y = st_coordinates(st_centroid(geometry))[,2],\n    x = ifelse(il_adi == \"istanbul\", x + 0.7, x),\n    y = ifelse(il_adi == \"istanbul\", y + 0.3, y)\n  ) |&gt;\n  ungroup()\n\n\np_uni &lt;- ggplot(etiketler_uni) +\n  geom_sf(aes(fill = mezun, geometry = geometry), color = \"white\", linewidth = 0.2) +\n  geom_text(data = subset(etiketler_uni, !is.na(etiket)),\n            aes(x = x, y = y, label = etiket),\n            size = 3, color = \"black\") +\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, na.value = \"grey90\") +\n  labs(\n    title = \"{closest_state} University Graduates (in Thousands)\",\n    fill = \"Thousand People\"\n  ) +\n  theme_minimal() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\nShow the code\nanim_save(\"uni.gif\", animate(p_uni, width = 800, height = 600, fps = 2, duration = 10))\nlibrary(magick)\nimage_read(\"uni.gif\") %&gt;%\n  .[length(.)] %&gt;%\n  image_convert(format = \"png\", type = \"truecolor\") %&gt;%\n  image_write(\"uni.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.7.3 Master’s Heat Map\n\n\nShow the code\nedu_master &lt;- read_excel(\"tidy_education.xlsx\") |&gt;\n  mutate(\n    Province = stri_trans_general(Province, \"Latin-ASCII\"),\n    Province = tolower(trimws(Province))\n  ) |&gt;\n  filter(Province != \"turkiye\") |&gt;\n  select(Year, il_adi = Province, mezun = Master_Total) |&gt;\n  mutate(mezun = mezun / 1000)\n\nharita_master &lt;- expand.grid(il_adi = unique(turkiye$il_adi), Year = unique(edu_master$Year)) |&gt;\n  left_join(edu_master, by = c(\"il_adi\", \"Year\")) |&gt;\n  left_join(turkiye, by = \"il_adi\") |&gt;\n  st_as_sf()\n\netiketler_master &lt;- harita_master |&gt;\n  group_by(Year) |&gt;\n  mutate(\n    sira = rank(mezun, ties.method = \"first\"),\n       etiket = case_when(\n  sira &lt;= 5 | sira &gt;= (n() - 4) ~ ifelse(mezun &lt; 1, \n                                        as.character(round(mezun * 1000)), \n                                        paste0(round(mezun), \"k\")),\n  TRUE ~ NA_character_\n),\n    x = st_coordinates(st_centroid(geometry))[,1],\n    y = st_coordinates(st_centroid(geometry))[,2],\n    x = ifelse(il_adi == \"istanbul\", x + 0.7, x),\n    y = ifelse(il_adi == \"istanbul\", y + 0.3, y)\n  ) |&gt;\n  ungroup()\n\np_master &lt;- ggplot(etiketler_master) +\n  geom_sf(aes(fill = mezun, geometry = geometry), color = \"white\", linewidth = 0.2) +\n  geom_text(data = subset(etiketler_master, !is.na(etiket)),\n            aes(x = x, y = y, label = etiket),\n            size = 3, color = \"black\") +\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, na.value = \"grey90\") +\n  labs(\n    title = \"{closest_state} Masters Graduates (in Thousands)\",\n    fill = \"Thousand People\"\n  ) +\n  theme_minimal() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\nShow the code\nanim_save(\"master.gif\", animate(p_master, width = 800, height = 600, fps = 2, duration = 10))\nlibrary(magick)\nimage_read(\"master.gif\") %&gt;%\n  .[length(.)] %&gt;%\n  image_convert(format = \"png\", type = \"truecolor\") %&gt;%\n  image_write(\"master.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.7.4 Doctorate Heat Map\n\n\nShow the code\nedu_phd &lt;- read_excel(\"tidy_education.xlsx\") |&gt;\n  mutate(\n    Province = stri_trans_general(Province, \"Latin-ASCII\"),\n    Province = tolower(trimws(Province))\n  ) |&gt;\n  filter(Province != \"turkiye\") |&gt;\n  select(Year, il_adi = Province, mezun = Doctorate_Total) |&gt;\n  mutate(mezun = mezun / 1000)\n\nharita_phd &lt;- expand.grid(il_adi = unique(turkiye$il_adi), Year = unique(edu_phd$Year)) |&gt;\n  left_join(edu_phd, by = c(\"il_adi\", \"Year\")) |&gt;\n  left_join(turkiye, by = \"il_adi\") |&gt;\n  st_as_sf()\n\netiketler_phd &lt;- harita_phd |&gt;\n  group_by(Year) |&gt;\n  mutate(\n    sira = rank(mezun, ties.method = \"first\"),\n    etiket = case_when(\n  sira &lt;= 5 | sira &gt;= (n() - 4) ~ ifelse(mezun &lt; 1, \n                                        as.character(round(mezun * 1000)), \n                                        paste0(round(mezun), \"k\")),\n  TRUE ~ NA_character_\n),\n    x = st_coordinates(st_centroid(geometry))[,1],\n    y = st_coordinates(st_centroid(geometry))[,2],\n    x = ifelse(il_adi == \"istanbul\", x + 0.7, x),\n    y = ifelse(il_adi == \"istanbul\", y + 0.3, y)\n  ) |&gt;\n  ungroup()\n\np_phd &lt;- ggplot(etiketler_phd) +\n  geom_sf(aes(fill = mezun, geometry = geometry), color = \"white\", linewidth = 0.2) +\n  geom_text(data = subset(etiketler_phd, !is.na(etiket)),\n            aes(x = x, y = y, label = etiket),\n            size = 3, color = \"black\") +\n  scale_fill_viridis_c(option = \"plasma\", direction = -1, na.value = \"grey90\") +\n  labs(\n    title = \"{closest_state} Doctorate Graduates (in Thousands)\",\n    fill = \"Thousand People\"\n  ) +\n  theme_minimal() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\nShow the code\nanim_save(\"phd.gif\", animate(p_phd, width = 800, height = 600, fps = 2, duration = 10))\nlibrary(magick)\nimage_read(\"phd.gif\") %&gt;%\n  .[length(.)] %&gt;%\n  image_convert(format = \"png\", type = \"truecolor\") %&gt;%\n  image_write(\"phd.png\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.8 Heat Maps of Cousin Marriage in Turkey\nThis animated map shows how cousin marriage rates change across provinces in Turkey over the years. Darker colors mean higher rates. We can see that eastern and southeastern provinces usually have the highest rates, while western provinces have much lower ones.\n\n\nShow the code\nharita_evlilik &lt;- expand.grid(il_adi = unique(turkiye$il_adi), Year = unique(combined_data$Year)) |&gt;\n  left_join(combined_data |&gt;\n              mutate(il_adi = tolower(stri_trans_general(Province, \"Latin-ASCII\"))),\n            by = c(\"il_adi\", \"Year\")) |&gt;\n  left_join(turkiye, by = \"il_adi\") |&gt;\n  st_as_sf()\n\netiketler_evlilik &lt;- harita_evlilik |&gt;\n  group_by(Year) |&gt;\n  mutate(\n    sira = rank(Proportion, ties.method = \"first\"),\n   etiket = case_when(\n  sira &lt;= 5 | sira &gt;= (n() - 4) ~ paste0(round(Proportion, 1), \"%\"),\n  TRUE ~ NA_character_\n),\n\n    x = st_coordinates(st_centroid(geometry))[,1],\n    y = st_coordinates(st_centroid(geometry))[,2],\n    x = ifelse(il_adi == \"istanbul\", x + 1.5, x),\n    y = ifelse(il_adi == \"istanbul\", y + 1.0, y)\n  ) |&gt;\n  ungroup()\n\np_evlilik &lt;- ggplot(etiketler_evlilik) +\n  geom_sf(aes(fill = Proportion, geometry = geometry), color = \"white\", linewidth = 0.2) +\n  geom_text(data = subset(etiketler_evlilik, !is.na(etiket)),\n            aes(x = x, y = y, label = etiket),\n            size = 3, color = \"black\") +\n  scale_fill_distiller(\n  palette = \"YlOrRd\",\n  direction = 1,\n  na.value = \"grey90\",\n  name = \"Proportion (%)\",\n  labels = waiver()\n)+\n  labs(\n    title = \"{closest_state} Consanguineous Marriage Rate by Province\",\n    fill = \"Proportion\"\n  ) +\n  theme_minimal() +\n  transition_states(Year, transition_length = 2, state_length = 1) +\n  ease_aes(\"cubic-in-out\")\n\n\n\n\nShow the code\nanim_save(\"consanguinity_rate.gif\", animate(p_evlilik, width = 800, height = 600, fps = 2, duration = 10))\nlibrary(magick)\nimage_read(\"consanguinity_rate.gif\") %&gt;%\n  .[length(.)] %&gt;%\n  image_convert(format = \"png\", type = \"truecolor\") %&gt;%\n  image_write(\"consanguinity_rate.png\")"
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.5 Results",
    "text": "3.5 Results\nxxxxxx"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "My Assignments",
    "section": "",
    "text": "On this page, I showcase the assignment I conducted for the Spring 2024-2025 EMU660 Decision Making with Analytics course.\nPlease use left menu to navigate through my assignments.\n\n\n\n Back to top",
    "crumbs": [
      "My Assignments"
    ]
  },
  {
    "objectID": "about.html#employements",
    "href": "about.html#employements",
    "title": "About Me",
    "section": "Employements",
    "text": "Employements\n\nRoketsan, Engineer, 2024 - ongoing\nBeko, Engineer, 2024"
  },
  {
    "objectID": "about.html#internships",
    "href": "about.html#internships",
    "title": "About Me",
    "section": "Internships",
    "text": "Internships\n\nHavelsan, Intern, 2022\nRoketsan, Intern, 2023"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/assignments/assignment-1.html",
    "href": "docs/assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "1 + 1\n\n[1] 2\n\n\nMy first assignment has two parts."
  },
  {
    "objectID": "assignments/assignment-1.html",
    "href": "assignments/assignment-1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "My first assignment has three parts.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-2.html",
    "href": "assignments/assignment-2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "Assignment 2\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignment 2"
    ]
  },
  {
    "objectID": "YagmurYunusASLAN.html",
    "href": "YagmurYunusASLAN.html",
    "title": "Welcome to My Analytics Lab, Please Take a Seat",
    "section": "",
    "text": "My name is Yagmur Yunus ASLAN.\nIn short I,\nAfter graduating from TOBB University of Economics and Technology with a grade point average of 3.53, I worked as a long-term intern at Havelsan and Roketsan for about 1 year, participating in business process modeling, defense industry, and space systems projects. Then I am now an engineer at Roketsan and studying MSc at Hacettepe University. I am proficient at the intermediate-upper level in English, and I have Spanish proficiency at the basic level. As an engineer constantly learning and improving himself, I am excellent at analytical thinking and problem-solving.\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "# Education"
  },
  {
    "objectID": "assignments/assignment-1.html#achoose-a-video-from-one-of-the-following-sources-and-provide-a-brief-summary",
    "href": "assignments/assignment-1.html#achoose-a-video-from-one-of-the-following-sources-and-provide-a-brief-summary",
    "title": "Assignment 1",
    "section": "(a)Choose a video from one of the following sources and provide a brief summary",
    "text": "(a)Choose a video from one of the following sources and provide a brief summary\nI chose “Veri Bilimi ve Endustri Muhendisligi Uzerine Sohbetler - Mustafa Baydogan & Erdi Dasdemir”\nAssoc. Prof. Dr. Mustafa Gokce Baydogan participated as a guest in the EMU430 Data Analytics course, which was newly added to the curriculum. He is a faculty member in the Department of Industrial Engineering at Bogazici University and the founder of Algopoly Software and Consultancy Joint Stock Company, based in Istanbul. With a strong academic background, he has worked on large-scale data mining, time series analysis, pattern exploration, and operations research using data science tools.\nTo convey his perspective to the students, he shared his answer to the question “Can the curvature of wet timber be estimated from its pictures?”, a problem he had worked on before.\nIn order to prevent the timber from being patched after the drying process, a pre-drying action is planned based on analyzing the wet timber images using image processing and machine learning techniques. The straight version of the timber used in home construction can be sold for 10 dollars, while the curved version is sold for only 2 dollars. If this problem is solved, a 5% increase in income is targeted.\nWhen addressing the problem, the root causes of warping were first investigated. Then, the factors that could cause warping were extracted from pictures taken before drying. These factors were processed and fed into learning algorithms, enabling the identification of timber pieces likely to warp. Filtering techniques helped interpret the locations and causes of warping.\nNext, to explain the importance of forecast accuracy in decision-making, he discussed how incorrect consumption forecasts in the electricity market can cause imbalances and how these effects can be mitigated.\nFollowing this, he explored the working principles of product ranking algorithms designed by e-commerce companies to optimize their revenue. The proposed ranking system arranges products based on the probability of purchase according to the buyer’s profile, and the discussion included how different ranking algorithms could be incorporated.\nFinally, to broaden students’ perspectives, he introduced physics-informed machine learning and machine learning for optimization.\nThe lesson concluded with a Q&A session.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#bexplore-statistical-summaries-with-custom-functions-and-iteration-methods",
    "href": "assignments/assignment-1.html#bexplore-statistical-summaries-with-custom-functions-and-iteration-methods",
    "title": "Assignment 1",
    "section": "(b)Explore Statistical Summaries with Custom Functions and Iteration Methods",
    "text": "(b)Explore Statistical Summaries with Custom Functions and Iteration Methods\nFirstly write a custom summary function. The function should be named compute_stats. It should take a numeric vector as input. The function should return a named list containing the mean, median, variance, interquartile range (IQR), minimum, and maximum of the input.\n\ncompute_stats&lt;- function(x){\n  if(is.numeric(x)){\n    k&lt;- list(\n      MEAN = mean(x),\n      MEDIAN = median(x),\n      VARIANCE = var(x),\n      IQR = IQR(x),\n      MIN = min(x),\n      MAX = max(x)\n      )\n    return (k)\n  }else print(\"Input was not numeric\")\n}\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\ncompute_stats(mtcars$mpg)\n\n$MEAN\n[1] 20.09062\n\n$MEDIAN\n[1] 19.2\n\n$VARIANCE\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$MIN\n[1] 10.4\n\n$MAX\n[1] 33.9\n\n\nSecondly, Applying the Function Using a Loop: Use a for loop to iterate over all numerical columns of the dataset. Within the loop, apply the compute_stats function to each column. Print the computed statistics, ensuring the column name appears in the output.\n\nfor(colonnames in colnames(mtcars)){\n  \n  colondata &lt;- mtcars[[colonnames]]\n  \n  print(colonnames)\n  print(compute_stats(colondata))\n  \n}\n\n[1] \"mpg\"\n$MEAN\n[1] 20.09062\n\n$MEDIAN\n[1] 19.2\n\n$VARIANCE\n[1] 36.3241\n\n$IQR\n[1] 7.375\n\n$MIN\n[1] 10.4\n\n$MAX\n[1] 33.9\n\n[1] \"cyl\"\n$MEAN\n[1] 6.1875\n\n$MEDIAN\n[1] 6\n\n$VARIANCE\n[1] 3.189516\n\n$IQR\n[1] 4\n\n$MIN\n[1] 4\n\n$MAX\n[1] 8\n\n[1] \"disp\"\n$MEAN\n[1] 230.7219\n\n$MEDIAN\n[1] 196.3\n\n$VARIANCE\n[1] 15360.8\n\n$IQR\n[1] 205.175\n\n$MIN\n[1] 71.1\n\n$MAX\n[1] 472\n\n[1] \"hp\"\n$MEAN\n[1] 146.6875\n\n$MEDIAN\n[1] 123\n\n$VARIANCE\n[1] 4700.867\n\n$IQR\n[1] 83.5\n\n$MIN\n[1] 52\n\n$MAX\n[1] 335\n\n[1] \"drat\"\n$MEAN\n[1] 3.596563\n\n$MEDIAN\n[1] 3.695\n\n$VARIANCE\n[1] 0.2858814\n\n$IQR\n[1] 0.84\n\n$MIN\n[1] 2.76\n\n$MAX\n[1] 4.93\n\n[1] \"wt\"\n$MEAN\n[1] 3.21725\n\n$MEDIAN\n[1] 3.325\n\n$VARIANCE\n[1] 0.957379\n\n$IQR\n[1] 1.02875\n\n$MIN\n[1] 1.513\n\n$MAX\n[1] 5.424\n\n[1] \"qsec\"\n$MEAN\n[1] 17.84875\n\n$MEDIAN\n[1] 17.71\n\n$VARIANCE\n[1] 3.193166\n\n$IQR\n[1] 2.0075\n\n$MIN\n[1] 14.5\n\n$MAX\n[1] 22.9\n\n[1] \"vs\"\n$MEAN\n[1] 0.4375\n\n$MEDIAN\n[1] 0\n\n$VARIANCE\n[1] 0.2540323\n\n$IQR\n[1] 1\n\n$MIN\n[1] 0\n\n$MAX\n[1] 1\n\n[1] \"am\"\n$MEAN\n[1] 0.40625\n\n$MEDIAN\n[1] 0\n\n$VARIANCE\n[1] 0.2489919\n\n$IQR\n[1] 1\n\n$MIN\n[1] 0\n\n$MAX\n[1] 1\n\n[1] \"gear\"\n$MEAN\n[1] 3.6875\n\n$MEDIAN\n[1] 4\n\n$VARIANCE\n[1] 0.5443548\n\n$IQR\n[1] 1\n\n$MIN\n[1] 3\n\n$MAX\n[1] 5\n\n[1] \"carb\"\n$MEAN\n[1] 2.8125\n\n$MEDIAN\n[1] 2\n\n$VARIANCE\n[1] 2.608871\n\n$IQR\n[1] 2\n\n$MIN\n[1] 1\n\n$MAX\n[1] 8\n\n\nLastly, analternative approach with sapply and apply: Instead of a for loop, use the sapply function to apply compute_stats across all numerical columns.\n\nusing_sapply &lt;- sapply(mtcars, function(col) {\n  if (is.numeric(col)){\n    compute_stats(col)\n  }\n})\nusing_sapply\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nMEAN     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nMEDIAN   19.2     6        196.3    123      3.695     3.325    17.71   \nVARIANCE 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nMIN      10.4     4        71.1     52       2.76      1.513    14.5    \nMAX      33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nMEAN     0.4375    0.40625   3.6875    2.8125  \nMEDIAN   0         0         4         2       \nVARIANCE 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nMIN      0         0         3         1       \nMAX      1         1         5         8       \n\n\nUse the apply function with the appropriate margin to apply your function across all columns of the matrix.\n\nmatrix &lt;- as.matrix(mtcars)\n\nusing_apply &lt;- apply(matrix, 2, compute_stats)\n\nusing_apply\n\n$mpg\n$mpg$MEAN\n[1] 20.09062\n\n$mpg$MEDIAN\n[1] 19.2\n\n$mpg$VARIANCE\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$MIN\n[1] 10.4\n\n$mpg$MAX\n[1] 33.9\n\n\n$cyl\n$cyl$MEAN\n[1] 6.1875\n\n$cyl$MEDIAN\n[1] 6\n\n$cyl$VARIANCE\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$MIN\n[1] 4\n\n$cyl$MAX\n[1] 8\n\n\n$disp\n$disp$MEAN\n[1] 230.7219\n\n$disp$MEDIAN\n[1] 196.3\n\n$disp$VARIANCE\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$MIN\n[1] 71.1\n\n$disp$MAX\n[1] 472\n\n\n$hp\n$hp$MEAN\n[1] 146.6875\n\n$hp$MEDIAN\n[1] 123\n\n$hp$VARIANCE\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$MIN\n[1] 52\n\n$hp$MAX\n[1] 335\n\n\n$drat\n$drat$MEAN\n[1] 3.596563\n\n$drat$MEDIAN\n[1] 3.695\n\n$drat$VARIANCE\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$MIN\n[1] 2.76\n\n$drat$MAX\n[1] 4.93\n\n\n$wt\n$wt$MEAN\n[1] 3.21725\n\n$wt$MEDIAN\n[1] 3.325\n\n$wt$VARIANCE\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$MIN\n[1] 1.513\n\n$wt$MAX\n[1] 5.424\n\n\n$qsec\n$qsec$MEAN\n[1] 17.84875\n\n$qsec$MEDIAN\n[1] 17.71\n\n$qsec$VARIANCE\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$MIN\n[1] 14.5\n\n$qsec$MAX\n[1] 22.9\n\n\n$vs\n$vs$MEAN\n[1] 0.4375\n\n$vs$MEDIAN\n[1] 0\n\n$vs$VARIANCE\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$MIN\n[1] 0\n\n$vs$MAX\n[1] 1\n\n\n$am\n$am$MEAN\n[1] 0.40625\n\n$am$MEDIAN\n[1] 0\n\n$am$VARIANCE\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$MIN\n[1] 0\n\n$am$MAX\n[1] 1\n\n\n$gear\n$gear$MEAN\n[1] 3.6875\n\n$gear$MEDIAN\n[1] 4\n\n$gear$VARIANCE\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$MIN\n[1] 3\n\n$gear$MAX\n[1] 5\n\n\n$carb\n$carb$MEAN\n[1] 2.8125\n\n$carb$MEDIAN\n[1] 2\n\n$carb$VARIANCE\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$MIN\n[1] 1\n\n$carb$MAX\n[1] 8",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#c",
    "href": "assignments/assignment-1.html#c",
    "title": "Assignment 1",
    "section": "(c)",
    "text": "(c)",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#cload-the-na_example-dataset-from-the-dslabs-package.",
    "href": "assignments/assignment-1.html#cload-the-na_example-dataset-from-the-dslabs-package.",
    "title": "Assignment 1",
    "section": "(c)Load the ???na_example??? dataset from the dslabs package.",
    "text": "(c)Load the ???na_example??? dataset from the dslabs package.\nDisplay the dataset contents , including any NA (missing) values present.\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nReport the total count of NA values found within the dataset and the index positions of all NA values in the dataset.\n\nsum(is.na(na_example))\n\n[1] 145\n\nwhich(is.na(na_example))\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n\nCompute and display the mean and standard deviation of the dataset before handling missing values (ignore NAs in calculations)\n\nmean(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 2.301754\n\nsd(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 1.22338\n\n\nHandling Missing Values- create two modified versions of the dataset:\nVersion 1: Replace all NA values with the median of the non-missing values.\n\nversion1&lt;-replace(na_example,which(is.na(na_example)),median(na_example,na.rm = TRUE))\nversion1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nVersion 2: Replace all NA values with a randomly selected non-missing value from the dataset.\n\nna_remove &lt;- na_example[which(!is.na(na_example))]\nversion2&lt;-replace(na_example,which(is.na(na_example)),sample(na_remove, sum(is.na(na_example))))\nversion2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 1 2 2 1 4 1 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 4 2 1 1 5 1 3 1 2 4 4 7 3 2 2 3 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 1 3 1 1 1 5 1 2 1 3 5 3 2 2 2 2 6 3 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 3 1 1 2 2 1 3 1 1 5 2 1 2 4 1 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 2 4 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 1 2 1 1 4 3 5 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 2 3 3 1 1 1 5 2 3 3 2 4 1 3 2 5 1 2 3\n [260] 4 6 2 2 2 2 2 1 2 6 3 3 2 2 4 3 1 4 2 1 2 4 3 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 1 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 3 1 2 1 1 3 3 1 1 1 3 5 3 2 3 4 1 4 3 1 1 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 3 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 1 3 3 1 2 4 1 2 1 1 4 2 1 4 4 5\n [408] 1 2 5 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 1 4 4 4 1 1 1 4\n [445] 3 5 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 1 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 1 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 3 1 4 1 2 4 1 3 2 1 1 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 1 2 3 2 1 2 1 1 1 2 2 3 1 5 2 1 2 5 3 2 2 2 1 5 3 2 3 1 1 3 1 2\n [593] 2 2 1 2 2 4 3 6 1 2 3 1 1 2 2 3 3 3 2 3 3 4 2 1 2 2 4 3 1 1 2 2 3 1 1 1 3\n [630] 3 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 3 1 4 3 2 2 2 3 2 4 2 2 4 3\n [667] 2 5 6 3 3 1 4 4 2 1 2 1 6 1 3 3 2 1 1 6 5 1 5 1 1 2 6 2 3 4 1 3 1 2 1 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 4 3 2 4 3 3 3\n [741] 2 4 2 4 2 3 4 2 2 1 2 2 4 3 2 1 2 3 1 3 4 2 1 2 1 2 1 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 4 1 3 1 2 3 1 2 1 2 1 3 1 3 2 3 2 2 2 1 4 2 1 1 2 2 4 2 3 2 3\n [852] 1 5 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 1 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 4 3\n [889] 3 2 2 7 1 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 3 1 1 3 2 1 1 1 2 2 2 2 3 3 2 1 3\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 4 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 4 3 2 1 3 1 1 2 5 3 5 1 1 4 3 2 1 3 1 1 2 4 3 3 3 3 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\nCompare the Results: Compute the mean and standard deviation of both modified datasets. Compare these statistics with those from the original dataset (before handling NAs). Briefly explain which method seems more appropriate for handling missing data in this case.\n\nsd(na_example, na.rm=TRUE)\n\n[1] 1.22338\n\nsd(version1)\n\n[1] 1.136102\n\nsd(version2)\n\n[1] 1.230249\n\n\nIt seems that it is more appropriate for handling missing data to write the most repeated value used in version 1 instead of NA, as it reduces the standard deviation of the data.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "assignments/assignment-1.html#cload-the-.na_example.-dataset-from-the-dslabs-package.",
    "href": "assignments/assignment-1.html#cload-the-.na_example.-dataset-from-the-dslabs-package.",
    "title": "Assignment 1",
    "section": "(c)Load the .na_example. dataset from the dslabs package.",
    "text": "(c)Load the .na_example. dataset from the dslabs package.\nDisplay the dataset contents , including any NA (missing) values present.\n\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\nReport the total count of NA values found within the dataset and the index positions of all NA values in the dataset.\n\nsum(is.na(na_example))\n\n[1] 145\n\nwhich(is.na(na_example))\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n\nCompute and display the mean and standard deviation of the dataset before handling missing values (ignore NAs in calculations)\n\nmean(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 2.301754\n\nsd(na_example, na.rm=TRUE) #default na.rm=FALSE\n\n[1] 1.22338\n\n\nHandling Missing Values- create two modified versions of the dataset:\nVersion 1: Replace all NA values with the median of the non-missing values.\n\nversion1&lt;-replace(na_example,which(is.na(na_example)),median(na_example,na.rm = TRUE))\nversion1\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\nVersion 2: Replace all NA values with a randomly selected non-missing value from the dataset.\n\nna_remove &lt;- na_example[which(!is.na(na_example))]\nversion2&lt;-replace(na_example,which(is.na(na_example)),sample(na_remove, sum(is.na(na_example))))\nversion2\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 3 2 2 1 4 1 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 1 2 1 1 1 5 1 3 1 2 4 4 7 3 2 1 4 1 3 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 5 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 1 4 3 4 3 1 2 1 2 3 2 2 1 5 1 2 1 3 5 3 2 2 1 5 1 2 3 5 3 1 1 4\n [149] 2 4 3 3 4 2 3 2 6 3 1 1 2 2 1 3 1 1 5 2 1 2 4 3 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 1 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 3 1 3 1 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 5 5 1 4 2 3 3 3 1 1 5 2 3 3 2 4 2 3 2 5 3 2 3\n [260] 4 6 2 2 2 1 2 1 2 6 3 3 2 2 4 3 1 4 2 3 2 4 1 6 2 3 1 1 2 2 3 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 3 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 4 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 3 2 2 1 2 1 1 4 2 1 4 4 3\n [408] 1 2 1 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 3 4 4 4 1 1 3 4\n [445] 3 1 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 1 3 1 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 1 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 4 1 4 1 2 4 1 3 2 3 1 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 7 1 1\n [556] 3 1 3 2 4 1 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 3 3 2 2 2 1 5 3 2 3 1 4 3 1 2\n [593] 2 2 1 2 2 4 3 6 1 2 3 1 1 2 2 3 2 3 2 3 3 4 2 2 2 5 4 2 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 2 7 1 3 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 1 1 1 4 3 2 2 2 3 2 4 2 2 4 4\n [667] 1 1 6 3 3 1 4 4 2 1 4 1 6 3 3 3 2 1 1 6 3 1 5 1 1 2 6 2 1 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 1 2 4 4 3 3 3\n [741] 2 4 2 4 2 2 3 3 2 1 1 2 4 3 2 1 2 3 1 3 4 3 1 2 1 2 1 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 4 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 1 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 4 1 2 4 2 4 1 3\n [852] 1 2 5 5 2 2 2 3 2 1 3 1 3 2 4 2 4 1 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 1 3\n [889] 3 2 2 1 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 3 1 3 3 2 1 1 1 2 1 2 2 3 3 2 1 3\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 1\n [963] 1 2 3 1 2 3 3 1 1 2 5 3 5 1 1 4 3 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 5\n[1000] 2\n\n\nCompare the Results: Compute the mean and standard deviation of both modified datasets. Compare these statistics with those from the original dataset (before handling NAs). Briefly explain which method seems more appropriate for handling missing data in this case.\n\nsd(na_example, na.rm=TRUE)\n\n[1] 1.22338\n\nsd(version1)\n\n[1] 1.136102\n\nsd(version2)\n\n[1] 1.216831\n\n\nIt seems that it is more appropriate for handling missing data to write the most repeated value used in version 1 instead of NA, as it reduces the standard deviation of the data.",
    "crumbs": [
      "Assignment 1"
    ]
  },
  {
    "objectID": "project.html#regression-analysis",
    "href": "project.html#regression-analysis",
    "title": "The Relationship Between Educational Level and The Number of First Cousin Marriages",
    "section": "3.4 Regression Analysis",
    "text": "3.4 Regression Analysis\nIn this analysis, we wanted to see if education levels have an effect on cousin marriage rates in Turkey. For this, we built a regression model where the cousin marriage rate is the outcome, and the shares of high school, university, master???s, and PhD graduates are the predictors.\nThe results show that provinces with a higher percentage of only high school graduates tend to have higher cousin marriage rates. On the other hand, places with more universities, master???s, and especially PhD graduates tend to have lower rates.\nThis supports the idea that as education level increases, cousin marriage becomes less common. While the model doesn???t explain everything (since cultural factors also matter), it still gives a clear overall pattern.\n\n\nShow the code\nlibrary(dplyr)\ncombined_data &lt;- combined_data |&gt;\n  mutate(\n    total_edu = High_school_Total + Universities_Total + Master_Total + Doctorate_Total,\n    hs_ratio = High_school_Total / total_edu,\n    uni_ratio = Universities_Total / total_edu,\n    master_ratio = Master_Total / total_edu,\n    phd_ratio = Doctorate_Total / total_edu\n  )\n\nmodel &lt;- lm(Proportion ~ hs_ratio + uni_ratio + master_ratio + phd_ratio, data = combined_data)\nsummary(model)\n\n\n\nCall:\nlm(formula = Proportion ~ hs_ratio + uni_ratio + master_ratio + \n    phd_ratio, data = combined_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.4538 -2.4797 -0.8696  1.5759 12.2465 \n\nCoefficients: (1 not defined because of singularities)\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -543.47      54.89  -9.901   &lt;2e-16 ***\nhs_ratio       557.66      55.02  10.136   &lt;2e-16 ***\nuni_ratio      539.25      55.12   9.784   &lt;2e-16 ***\nmaster_ratio   563.43      63.95   8.811   &lt;2e-16 ***\nphd_ratio          NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.441 on 1144 degrees of freedom\nMultiple R-squared:  0.2071,    Adjusted R-squared:  0.2051 \nF-statistic: 99.62 on 3 and 1144 DF,  p-value: &lt; 2.2e-16\n\n\nIn this regression model, we examined how the relative share of graduates at different education levels affects cousin marriage rates. The dependent variable is the cousin marriage proportion, and the predictors are the ratios of high school, university, master???s, and PhD graduates among all graduates.\nThe results show that high school, university, and masters graduate shares are all positively associated with cousin marriage rate. Surprisingly, the coefficient signs are all positive, but the interpretation depends on the structure of the ratios.\nThe coefficient for phd_ratio is marked as NA due to perfect multicollinearity ??? meaning its value is automatically determined by the others and cannot be estimated separately. This happens because the ratios sum to 1, so one of them must be dropped from the model.\nThe model explains around 20.5% of the variation in cousin marriage rates (Adjusted R?? = 0.205), which is reasonable given that cultural and regional factors are not included. The model is statistically significant overall (p &lt; 0.001)."
  }
]